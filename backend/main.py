"""
ì£¼ì‹ ë‚ ì”¨ ì˜ˆë³´íŒ - FastAPI ë©”ì¸ ì„œë²„ (ê°œì„ ëœ ë²„ì „)
"""
from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import asyncio
import logging
import structlog
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import time

from config import settings
from models import *
from data_pipeline import DataPipeline
from score_calculator import FundamentalScorer
from ml_predictor import StockPredictor
from cache_manager import CacheManager
from exceptions import *

# êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
data_pipeline = None
scorer = None
predictor = None
cache = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    global data_pipeline, scorer, predictor, cache
    
    logger.info("application_startup", env=settings.env)
    
    try:
        # ì„¤ì • ê²€ì¦
        settings.validate_settings()
        
        # ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
        cache = CacheManager(settings.cache_db_path)
        await cache.initialize()
        
        data_pipeline = DataPipeline(cache)
        scorer = FundamentalScorer()
        predictor = StockPredictor()
        
        # ML ëª¨ë¸ ë¡œë“œ
        await predictor.load_models()
        
        # ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ (ë°±ê·¸ë¼ìš´ë“œ)
        asyncio.create_task(initial_data_collection())
        
        logger.info("application_startup_complete")
        
    except Exception as e:
        logger.error("application_startup_failed", error=str(e))
        raise
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    logger.info("application_shutdown")
    if cache:
        await cache.close()

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="ì£¼ì‹ ë‚ ì”¨ ì˜ˆë³´íŒ API",
    description="AI ê¸°ë°˜ ì£¼ì‹ ìƒìŠ¹/í•˜ë½ í™•ë¥  ì˜ˆì¸¡ ì„œë¹„ìŠ¤",
    version="2.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# ì—ëŸ¬ í•¸ë“¤ëŸ¬
@app.exception_handler(StockWeatherException)
async def stock_weather_exception_handler(request: Request, exc: StockWeatherException):
    logger.error("custom_exception", exception=str(exc), path=request.url.path)
    return JSONResponse(
        status_code=400,
        content={"detail": str(exc), "type": exc.__class__.__name__}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error("unhandled_exception", exception=str(exc), path=request.url.path)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "type": "InternalError"}
    )

# ë¯¸ë“¤ì›¨ì–´: ìš”ì²­ ë¡œê¹…
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(
        "request_processed",
        path=request.url.path,
        method=request.method,
        status_code=response.status_code,
        process_time=process_time
    )
    
    return response

async def initial_data_collection():
    """ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘"""
    try:
        logger.info("initial_data_collection_started")
        
        # í•œêµ­ ì£¼ì‹
        kr_tickers = await data_pipeline.get_kr_tickers()
        await data_pipeline.fetch_batch_data(kr_tickers[:100], market=Market.KR)
        
        # ë¯¸êµ­ ì£¼ì‹
        us_tickers = await data_pipeline.get_us_tickers()
        await data_pipeline.fetch_batch_data(us_tickers[:100], market=Market.US)
        
        logger.info("initial_data_collection_completed")
    except Exception as e:
        logger.error("initial_data_collection_failed", error=str(e))

@app.get("/")
async def root():
    """API ì •ë³´"""
    return {
        "service": "Stock Weather Dashboard",
        "version": "2.0.0",
        "environment": settings.env,
        "endpoints": {
            "/rankings": "ìƒìŠ¹/í•˜ë½ í™•ë¥  ë­í‚¹",
            "/detail/{ticker}": "ì¢…ëª© ìƒì„¸ ì •ë³´",
            "/sectors": "ì„¹í„°ë³„ ë‚ ì”¨ ì§€ë„",
            "/health": "ì„œë²„ ìƒíƒœ"
        }
    }

@app.get("/rankings", response_model=RankingsResponse)
async def get_rankings(
    market: Market = Market.ALL,
    limit: int = 20,
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """ìƒìŠ¹/í•˜ë½ í™•ë¥  ë­í‚¹ ì¡°íšŒ"""
    logger.info("rankings_requested", market=market, limit=limit)
    
    try:
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = cache.generate_cache_key(f"rankings_{market}_{limit}", "rankings")
        cached = await cache.get(cache_key)
        
        if cached and not await should_refresh_cache(cached):
            logger.info("rankings_from_cache", cache_key=cache_key)
            return RankingsResponse(**cached)
        
        # ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ë° ì˜ˆì¸¡
        stocks = await get_stock_predictions(market)
        
        # ìƒìŠ¹/í•˜ë½ í™•ë¥ ë¡œ ì •ë ¬
        top_gainers = sorted(stocks, key=lambda x: x['probability'], reverse=True)[:limit]
        top_losers = sorted(stocks, key=lambda x: x['probability'])[:limit]
        
        # ë‚ ì”¨ ì•„ì´ì½˜ ì¶”ê°€
        for stock in top_gainers + top_losers:
            stock['weather_icon'] = get_weather_icon(stock['probability'])
        
        rankings_data = {
            "top_gainers": [StockRanking(**s) for s in top_gainers],
            "top_losers": [StockRanking(**s) for s in top_losers],
            "updated_at": datetime.now()
        }
        
        # ìºì‹œ ì—…ë°ì´íŠ¸ (ë°±ê·¸ë¼ìš´ë“œ)
        background_tasks.add_task(
            cache.set, 
            cache_key, 
            rankings_data, 
            ttl=settings.cache_ttl
        )
        
        logger.info("rankings_generated", gainers_count=len(top_gainers), losers_count=len(top_losers))
        return RankingsResponse(**rankings_data)
        
    except Exception as e:
        logger.error("rankings_error", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/detail/{ticker}", response_model=DetailedStock)
async def get_stock_detail(ticker: str):
    """ì¢…ëª© ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    logger.info("stock_detail_requested", ticker=ticker)
    
    try:
        # ìºì‹œ í™•ì¸
        cache_key = cache.generate_cache_key(ticker, "detail")
        cached = await cache.get(cache_key)
        
        if cached and (datetime.now() - cached['last_updated']).seconds < settings.cache_freshness:
            logger.info("stock_detail_from_cache", ticker=ticker)
            return DetailedStock(**cached)
        
        # ë°ì´í„° ìˆ˜ì§‘
        stock_data = await data_pipeline.get_stock_data(ticker)
        
        if not stock_data:
            raise HTTPException(status_code=404, detail="ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # í€ë”ë©˜í„¸ ë¶„ì„
        fundamental_score, breakdown = await scorer.calculate_detailed_score(stock_data)
        
        # ê°€ê²© ì˜ˆì¸¡
        prediction = await predictor.predict_single(stock_data)
        
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        technical = calculate_technical_indicators(stock_data.get('price_history', []))
        
        # ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (ì˜µì…˜)
        news_sentiment = await analyze_news_sentiment(ticker) if ticker.endswith('.KS') else None
        
        # ê°€ê²© ì´ë ¥ ë³€í™˜
        price_history = [
            PriceHistory(**p) for p in stock_data.get('price_history', [])[-120:]
        ]
        
        detailed_info = DetailedStock(
            ticker=ticker,
            name=stock_data.get('name', ticker),
            sector=stock_data.get('sector', 'Unknown'),
            current_price=stock_data.get('current_price', 0),
            probability=prediction['probability'],
            expected_return=prediction['expected_return'],
            fundamental_breakdown=breakdown,
            price_history=price_history,
            news_sentiment=news_sentiment,
            technical_indicators=technical,
            last_updated=datetime.now()
        )
        
        # ìºì‹œ ì €ì¥
        await cache.set(cache_key, detailed_info.dict(), ttl=settings.cache_ttl)
        
        logger.info("stock_detail_generated", ticker=ticker)
        return detailed_info
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("stock_detail_error", ticker=ticker, error=str(e))
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/sectors")
async def get_sector_weather():
    """ì„¹í„°ë³„ ë‚ ì”¨ ì§€ë„"""
    logger.info("sector_weather_requested")
    
    try:
        # ì„¹í„°ë³„ í‰ê·  í™•ë¥  ê³„ì‚°
        sector_data = await data_pipeline.get_sector_aggregates()
        
        sector_weather = []
        for sector, data in sector_data.items():
            avg_probability = data.get('avg_probability', 0.5)
            weather = SectorWeather(
                sector=sector,
                probability=avg_probability,
                weather_icon=get_weather_icon(avg_probability),
                weather_desc=get_weather_description(avg_probability),
                stock_count=data.get('count', 0),
                top_stock=data.get('top_stock', '')
            )
            sector_weather.append(weather)
        
        result = {
            "sectors": sorted(sector_weather, key=lambda x: x.probability, reverse=True),
            "updated_at": datetime.now().isoformat()
        }
        
        logger.info("sector_weather_generated", sector_count=len(sector_weather))
        return result
        
    except Exception as e:
        logger.error("sector_weather_error", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        # ê° ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸
        checks = {
            "cache": await cache.health_check() if cache else False,
            "models": predictor.is_loaded if predictor else False,
            "data_pipeline": data_pipeline is not None
        }
        
        all_healthy = all(checks.values())
        
        return {
            "status": "healthy" if all_healthy else "unhealthy",
            "checks": checks,
            "timestamp": datetime.now().isoformat(),
            "environment": settings.env
        }
    except Exception as e:
        logger.error("health_check_error", error=str(e))
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# í—¬í¼ í•¨ìˆ˜ë“¤
async def should_refresh_cache(cached_data: Dict) -> bool:
    """ìºì‹œ ê°±ì‹  í•„ìš” ì—¬ë¶€ í™•ì¸"""
    if not cached_data:
        return True
    
    # ì—…ë°ì´íŠ¸ ì‹œê°„ í™•ì¸
    if 'updated_at' in cached_data:
        if isinstance(cached_data['updated_at'], str):
            updated_at = datetime.fromisoformat(cached_data['updated_at'])
        else:
            updated_at = cached_data['updated_at']
        
        age = (datetime.now() - updated_at).seconds
        
        # 3ì‹œê°„ TTL
        if age > settings.cache_ttl:
            return True
        
        # ìµœê·¼ 1ì‹œê°„ ì´ë‚´ ë°ì´í„°ëŠ” ìœ ì§€
        if age < settings.cache_freshness:
            return False
    
    # TODO: Â±5% ë³€ë™ ì²´í¬ êµ¬í˜„
    
    return False

async def get_stock_predictions(market: Market) -> List[Dict]:
    """ì£¼ì‹ ì˜ˆì¸¡ ë°ì´í„° ìƒì„±"""
    logger.info("generating_predictions", market=market)
    
    # í‹°ì»¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    if market == Market.KR:
        tickers = await data_pipeline.get_kr_tickers()
    elif market == Market.US:
        tickers = await data_pipeline.get_us_tickers()
    else:  # ALL
        kr_tickers = await data_pipeline.get_kr_tickers()
        us_tickers = await data_pipeline.get_us_tickers()
        tickers = kr_tickers[:50] + us_tickers[:50]
    
    predictions = []
    
    # ë°°ì¹˜ ì²˜ë¦¬
    for i in range(0, len(tickers), settings.batch_size):
        batch = tickers[i:i+settings.batch_size]
        batch_data = await data_pipeline.fetch_batch_data(batch, market)
        
        for ticker, data in batch_data.items():
            if data:
                try:
                    # í€ë”ë©˜í„¸ ìŠ¤ì½”ì–´
                    fundamental_score = await scorer.calculate_score(data)
                    
                    # ML ì˜ˆì¸¡
                    prediction = await predictor.predict_single(data)
                    
                    predictions.append({
                        "ticker": ticker,
                        "name": data.get('name', ticker),
                        "sector": data.get('sector', 'Unknown'),
                        "probability": prediction['probability'],
                        "expected_return": prediction['expected_return'],
                        "fundamental_score": fundamental_score,
                        "confidence": prediction.get('confidence', 0.5)
                    })
                except Exception as e:
                    logger.error("prediction_error", ticker=ticker, error=str(e))
    
    logger.info("predictions_generated", count=len(predictions))
    return predictions

def get_weather_icon(probability: float) -> str:
    """í™•ë¥ ì— ë”°ë¥¸ ë‚ ì”¨ ì•„ì´ì½˜"""
    if probability >= 0.7:
        return "â˜€ï¸"  # ë§‘ìŒ
    elif probability >= 0.6:
        return "ğŸŒ¤ï¸"  # ì•½ê°„ êµ¬ë¦„
    elif probability >= 0.4:
        return "â›…"  # êµ¬ë¦„ ë§ìŒ
    elif probability >= 0.3:
        return "ğŸŒ¥ï¸"  # íë¦¼
    else:
        return "ğŸŒ§ï¸"  # ë¹„

def get_weather_description(probability: float) -> str:
    """í™•ë¥ ì— ë”°ë¥¸ ë‚ ì”¨ ì„¤ëª…"""
    if probability >= 0.7:
        return "ë§‘ê³  í™”ì°½í•œ ìƒìŠ¹ì„¸ê°€ ì˜ˆìƒë©ë‹ˆë‹¤"
    elif probability >= 0.6:
        return "ëŒ€ì²´ë¡œ ë§‘ì€ ìƒìŠ¹ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤"
    elif probability >= 0.4:
        return "ë³€ë™ì„±ì´ ìˆëŠ” êµ¬ë¦„ ë‚€ ë‚ ì”¨ì…ë‹ˆë‹¤"
    elif probability >= 0.3:
        return "íë¦° ë‚ ì”¨ë¡œ ì¡°ì • ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤"
    else:
        return "ë¹„ ì˜¤ëŠ” ë‚ ì”¨, í•˜ë½ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤"

def calculate_technical_indicators(price_history: List[Dict]) -> Dict[str, float]:
    """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
    if len(price_history) < 20:
        return {
            "ma20": 0,
            "ma60": 0,
            "rsi": 50,
            "volatility": 0
        }
    
    try:
        closes = [p['close'] for p in price_history]
        
        # ì´ë™í‰ê· 
        ma20 = sum(closes[-20:]) / 20
        ma60 = sum(closes[-60:]) / 60 if len(closes) >= 60 else ma20
        
        # RSI
        rsi = calculate_rsi(closes)
        
        # ë³€ë™ì„±
        volatility = calculate_volatility(closes[-20:])
        
        return {
            "ma20": round(ma20, 2),
            "ma60": round(ma60, 2),
            "rsi": round(rsi, 2),
            "volatility": round(volatility, 2)
        }
    except Exception as e:
        logger.error("technical_indicators_error", error=str(e))
        return {
            "ma20": 0,
            "ma60": 0,
            "rsi": 50,
            "volatility": 0
        }

def calculate_rsi(prices: List[float], period: int = 14) -> float:
    """RSI ê³„ì‚°"""
    if len(prices) < period + 1:
        return 50.0
    
    gains = []
    losses = []
    
    for i in range(1, len(prices)):
        diff = prices[i] - prices[i-1]
        if diff > 0:
            gains.append(diff)
            losses.append(0)
        else:
            gains.append(0)
            losses.append(abs(diff))
    
    avg_gain = sum(gains[-period:]) / period if gains else 0
    avg_loss = sum(losses[-period:]) / period if losses else 0
    
    if avg_loss == 0:
        return 100.0
    
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    
    return rsi

def calculate_volatility(prices: List[float]) -> float:
    """ë³€ë™ì„± ê³„ì‚°"""
    import numpy as np
    
    if len(prices) < 2:
        return 0.0
    
    returns = [(prices[i] / prices[i-1] - 1) for i in range(1, len(prices))]
    return np.std(returns) * np.sqrt(252) * 100  # ì—°ìœ¨í™”

async def analyze_news_sentiment(ticker: str) -> float:
    """ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (í•œêµ­ ì£¼ì‹ë§Œ)"""
    # TODO: ì‹¤ì œ êµ¬í˜„ ì‹œ news_analyzer.py ëª¨ë“ˆ ì‚¬ìš©
    import random
    return random.uniform(-1, 1)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_config=None)
